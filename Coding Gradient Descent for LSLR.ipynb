{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#The Gradient Descent Algorithm \n","\n","This project is an implementation of the gradient descent algorithm for linear regression.\n","\n","It uses functions to calculate the gradient of the cost function and the cost function itself, with respect to the parameters w and b, and uses them to update the parameters during the training process.\n","\n","The algorithm stops after 10 iterations and the final values of w and b represent the parameters of the model that minimize the cost function. It also prints the current iteration number, current values of w and b, gradient, and cost for each iteration."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":416,"status":"ok","timestamp":1635538071481,"user":{"displayName":"Mohammad M. Ajallooeian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzJw16TibpMWad0qfsUyIGrTDe57T4dDk9V5jK4w=s64","userId":"03663965481620989481"},"user_tz":360},"id":"fSeeWOsyp_7Z"},"outputs":[],"source":["import numpy as np\n","import plotly.express as px"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":192,"status":"ok","timestamp":1635538110782,"user":{"displayName":"Mohammad M. Ajallooeian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzJw16TibpMWad0qfsUyIGrTDe57T4dDk9V5jK4w=s64","userId":"03663965481620989481"},"user_tz":360},"id":"pFeZseQlqC_5"},"outputs":[],"source":["X = np.array([0.5, 2.3, 2.9])\n","y = np.array([1.4, 1.9, 3.2])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":691,"status":"ok","timestamp":1635538112919,"user":{"displayName":"Mohammad M. Ajallooeian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzJw16TibpMWad0qfsUyIGrTDe57T4dDk9V5jK4w=s64","userId":"03663965481620989481"},"user_tz":360},"id":"IRVuE4PvqcDa","outputId":"a22918b6-b17e-438e-b234-74992bbb1290"},"outputs":[{"data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>\n","            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n","                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n","            <div id=\"bb71180b-4f99-44e5-8427-4be864b8faa3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n","            <script type=\"text/javascript\">\n","                \n","                    window.PLOTLYENV=window.PLOTLYENV || {};\n","                    \n","                if (document.getElementById(\"bb71180b-4f99-44e5-8427-4be864b8faa3\")) {\n","                    Plotly.newPlot(\n","                        'bb71180b-4f99-44e5-8427-4be864b8faa3',\n","                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"x=%{x}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [1.4, 1.9, 3.2], \"xaxis\": \"x\", \"yaxis\": \"y\"}],\n","                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"x\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0]}},\n","                        {\"responsive\": true}\n","                    ).then(function(){\n","                            \n","var gd = document.getElementById('bb71180b-4f99-44e5-8427-4be864b8faa3');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })\n","                };\n","                \n","            </script>\n","        </div>\n","</body>\n","</html>"]},"metadata":{},"output_type":"display_data"}],"source":["px.scatter(X.squeeze(), y.squeeze())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hHzSQYBHq2n8"},"source":["The equation $J = |\\hat{y} - y|^2$ is the cost function for a linear regression model, where $\\hat{y}$ is the predicted value and y is the actual value. The cost function is the mean squared error (MSE) between the predicted values and the actual values. It is used to measure the performance of the model. The goal of the gradient descent algorithm is to minimize this cost function.\n","\n","The equation $J = |wx + b - y|^2$ is the same cost function, but it is written in terms of the model parameters w and b, which represent the slope and y-intercept of the line, respectively."]},{"cell_type":"markdown","metadata":{},"source":["$J = |\\hat{y} - y|^2$\n","\n","$J = |wx + b - y|^2$\n","\n","$\\frac{\\mathrm{d}J}{\\mathrm{d}w}, \\frac{\\mathrm{d}J}{\\mathrm{d}b}$"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","Gradient Descent is a optimization algorithm used to minimize the cost function. The gradient of the cost function with respect to the parameters is calculated, and the parameters are updated in the direction of the negative gradient. The update rule for the parameters is given by:\n","\n","$w = w - \\alpha \\frac{\\partial J}{\\partial w}$\n","\n","$b = b - \\alpha \\frac{\\partial J}{\\partial b}$\n","\n","Where $\\alpha$ is the learning rate, a small value used to control the step size of the update.\n","\n","By updating the parameters in the direction of the negative gradient, the cost function is decreased at each iteration, and the algorithm will converge to the minimum of the cost function.\n","\n","Gradient descent is a optimization algorithm used to minimize the cost function, $J = |wx + b - y|^2$, of a linear regression model by updating the parameters w and b in the direction of the negative gradient of the cost function with respect to the parameters. The learning rate, $\\alpha$, is used to control the step size of the update."]},{"cell_type":"markdown","metadata":{"id":"LqyrvOBwrfgS"},"source":["$\\frac{\\mathrm{d}J}{\\mathrm{d}w} = 2 (wx + b - y)^{(2 - 1)} \\cdot \\frac{\\mathrm{d}(wx + b - y)}{\\mathrm{d}w}$\n","\n","$\\frac{\\mathrm{d}J}{\\mathrm{d}w} = 2 (wx + b - y) \\cdot (x + 0 - 0)$\n","\n","$\\frac{\\mathrm{d}J}{\\mathrm{d}w} = 2 (wx + b - y - y)x$\n","\n","$\\frac{\\mathrm{d}J}{\\mathrm{d}b} = 2 (wx + b - y)^{(2 - 1)} \\cdot \\frac{\\mathrm{d}(wx + b - y)}{\\mathrm{d}b}$\n","\n","$\\frac{\\mathrm{d}J}{\\mathrm{d}b} = 2 (wx + b - y) \\cdot (0 + 1 - 0)$\n","\n","$\\frac{\\mathrm{d}J}{\\mathrm{d}b} = 2 (wx + b - y - y)\\cdot 1$\n","\n","$\\frac{\\mathrm{d}J}{\\mathrm{d}b} = 2 (wx + b - y - y)$"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The above mathematical expressions are the gradient of the cost function $J = |wx + b - y|^2$ with respect to the parameters w and b. It is used to calculate the update of the parameters during the gradient descent algorithm.\n","\n","The first equation $\\frac{\\mathrm{d}J}{\\mathrm{d}w} = 2 (wx + b - y)^{(2 - 1)} \\cdot \\frac{\\mathrm{d}(wx + b - y)}{\\mathrm{d}w}$ is the gradient of the cost function with respect to the w parameter.\n","\n","The second equation $\\frac{\\mathrm{d}J}{\\mathrm{d}w} = 2 (wx + b - y) \\cdot (x + 0 - 0)$ is obtained by applying the chain rule to the first equation. This is used to simplify the expression by applying the derivative of the outer function and inner function.\n","\n","The third equation $\\frac{\\mathrm{d}J}{\\mathrm{d}w} = 2 (wx + b - y - y)x$ is obtained by simplifying the second equation by replacing the outer function with the cost function and inner function with the model equation.\n","\n","The fourth equation $\\frac{\\mathrm{d}J}{\\mathrm{d}b} = 2 (wx + b - y)^{(2 - 1)} \\cdot \\frac{\\mathrm{d}(wx + b - y)}{\\mathrm{d}b}$ is the gradient of the cost function with respect to the b parameter.\n","\n","The fifth equation $\\frac{\\mathrm{d}J}{\\mathrm{d}b} = 2 (wx + b - y) \\cdot (0 + 1 - 0)$ is obtained by applying the chain rule to the fourth equation.\n","\n","The sixth equation $\\frac{\\mathrm{d}J}{\\mathrm{d}b} = 2 (wx + b - y - y)\\cdot 1$ is obtained by simplifying the fifth equation by replacing the outer function with the cost function and inner function with the model equation.\n","\n","The seventh equation $\\frac{\\mathrm{d}J}{\\mathrm{d}b} = 2 (wx + b - y - y)$ is obtained by further simplifying the sixth equation by removing the unnecessary factor of 1.\n","\n","These mathematical expressions are used to calculate the gradient of the cost function with respect to the parameters w and b, which is used to update the parameters during the gradient descent algorithm."]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":207,"status":"ok","timestamp":1635539358390,"user":{"displayName":"Mohammad M. Ajallooeian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzJw16TibpMWad0qfsUyIGrTDe57T4dDk9V5jK4w=s64","userId":"03663965481620989481"},"user_tz":360},"id":"N9PvtY3XrejV"},"outputs":[],"source":["def dJdw(x, y, w, b):\n"," return 2 * ((w * x) + b - y) * x\n","def dJdb(x, y, w, b):\n"," return 2 * ((w * x) + b - y)\n","def loss(x, y, w, b):\n"," return ((w * x) + b - y) ** 2"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This code defines three functions: dJdw, dJdb and loss.\n","\n","The first function, dJdw(x, y, w, b), calculates the gradient of the cost function with respect to the parameter w, which is used to update the value of w during the gradient descent algorithm. It takes four input arguments: x (the input data), y (the actual value), w (the current value of the parameter w), and b (the current value of the parameter b). It returns the value of the gradient of the cost function with respect to w, calculated as 2 * ((w * x) + b - y) * x.\n","\n","The second function, dJdb(x, y, w, b), calculates the gradient of the cost function with respect to the parameter b, which is used to update the value of b during the gradient descent algorithm. It takes four input arguments: x (the input data), y (the actual value), w (the current value of the parameter w), and b (the current value of the parameter b). It returns the value of the gradient of the cost function with respect to b, calculated as 2 * ((w * x) + b - y)\n","\n","The third function, loss(x, y, w, b), calculates the value of the cost function which is used to measure the performance of the model. It takes four input arguments: x (the input data), y (the actual value), w (the current value of the parameter w), and b (the current value of the parameter b). It returns the value of the cost function, calculated as ((w * x) + b - y) ** 2"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":681,"status":"ok","timestamp":1635540243330,"user":{"displayName":"Mohammad M. Ajallooeian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzJw16TibpMWad0qfsUyIGrTDe57T4dDk9V5jK4w=s64","userId":"03663965481620989481"},"user_tz":360},"id":"7L1Cab9qqmG-","outputId":"d5475c52-f441-4c4b-ff02-73c1d6a3aa9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["iteration 0\n","w = 0\n","b = 0\n","gradient w.r.t. w = -28.699999999999996\n","gradient w.r.t. b = -13.0\n","cost = 15.810000000000002\n","\n","iteration 1\n","w = 2.8699999999999997\n","b = 1.3\n","gradient w.r.t. w = 66.19299999999998\n","gradient w.r.t. b = 27.517999999999997\n","cost = 79.04915499999998\n","\n","iteration 2\n","w = -3.7492999999999994\n","b = -1.4517999999999998\n","gradient w.r.t. w = -149.85599\n","gradient w.r.t. b = -64.45282\n","cost = 406.76298869149997\n","\n","iteration 3\n","w = 11.236299\n","b = 4.993482000000001\n","gradient w.r.t. w = 341.71843690000003\n","gradient w.r.t. b = 145.0547006\n","cost = 2104.1007599912614\n","\n","iteration 4\n","w = -22.93554469\n","b = -9.51198806\n","gradient w.r.t. w = -777.0383607350001\n","gradient w.r.t. b = -331.53713782600005\n","cost = 10894.45066115621\n","\n","iteration 5\n","w = 54.768291383500014\n","b = 23.641725722600007\n","gradient w.r.t. w = 1768.8510028372903\n","gradient w.r.t. b = 753.2088761075001\n","cost = 56418.266200525\n","\n","iteration 6\n","w = -122.11680890022902\n","b = -51.67916188815001\n","gradient w.r.t. w = -4024.9014138412995\n","gradient w.r.t. b = -1715.2065927915107\n","cost = 292178.3787483984\n","\n","iteration 7\n","w = 280.373332483901\n","b = 119.84149739100107\n","gradient w.r.t. w = 9159.909046558249\n","gradient w.r.t. b = 3902.3049746624774\n","cost = 1513139.6309822507\n","\n","iteration 8\n","w = -635.6175721719239\n","b = -270.3890000752467\n","gradient w.r.t. w = -20844.86486445449\n","gradient w.r.t. b = -8881.374323211412\n","cost = 7836288.610632909\n","\n","iteration 9\n","w = 1448.868914273525\n","b = 617.7484322458945\n","gradient w.r.t. w = 47437.07483583454\n","gradient w.r.t. b = 20210.596216193553\n","cost = 40582792.79677853\n","\n"]}],"source":["w = 0\n","b = 0\n","\n","step_size = 0.1\n"," \n","for j in range(10):\n","  print(\"iteration\", j)\n","  print(\"w =\", w)\n","  print(\"b =\", b)\n","\n","  sw = 0\n","  sb = 0\n","  for i in range(X.shape[0]):\n","    sw += dJdw(X[i], y[i], w, b)\n","    sb += dJdb(X[i], y[i], w, b)\n","\n","  print(\"gradient w.r.t. w =\", sw)\n","  print(\"gradient w.r.t. b =\", sb)\n","\n","  c = 0\n","  for i in range(X.shape[0]):\n","    c += loss(X[i], y[i], w, b)\n","  print(\"cost =\", c)\n","\n","  w -= step_size * sw # w_new = w_old - (0.05 * sw)\n","  b -= step_size * sb # b_new = b_old - (0.05 * bw)\n","\n","  print()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This code is an implementation of the gradient descent algorithm for linear regression. It initializes the model parameters w and b with the values of 0, and a step size of 0.1.\n","\n","It then performs 10 iterations of the algorithm, in each iteration:\n","\n","- It prints the current iteration number, current values of w and b.\n","- It initializes the variables sw and sb to 0, which will be used to store the sum of the gradients of the cost function with respect to w and b over all the data points.\n","- It loops over all the data points and for each data point it calls the function dJdw(X[i], y[i], w, b) and dJdb(X[i], y[i], w, b) to calculate the gradient of the cost function with respect to w and b respectively. It then adds these values to the variables sw and sb.\n","- It prints the values of sw and sb which are the sum of the gradients of the cost function with respect to w and b over all the data points.\n","- It initializes the variable c to 0, which will be used to store the sum of the cost function over all the data points.\n","- It loops over all the data points and for each data point it calls the function loss(X[i], y[i], w, b) to calculate the value of the cost function. It then adds this value to the variable c.\n","- It prints the value of c which is the sum of the cost function over all the data points.\n","- It updates the values of w and b by subtracting the product of step size and the gradient of the cost function with respect to w and b respectively.\n","\n","The algorithm stops after 10 iterations, and the final values of w and b represent the parameters of the model that minimize the cost function."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN3hnN+tgoqRxBcPyN5N8Lk","name":"Coding Gradient Descent for LSLR.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
